{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projekt_kmeans.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF1oHl6TFeb2"
      },
      "source": [
        "# **Importing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQbShRIrEObW",
        "outputId": "d7f73b3b-44f7-4643-a4c4-5315e43b989f"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas  as pd\r\n",
        "import math\r\n",
        "import keras\r\n",
        "import sklearn.model_selection\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import string\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 475,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeEpFsau6cqI"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTZMaCJp6czn"
      },
      "source": [
        "def tokenization(data):\r\n",
        "\r\n",
        "  vocabulary = {}\r\n",
        "  finalDocumentIndices = []\r\n",
        "\r\n",
        "  for i in data.keys():\r\n",
        "    text = data[i]\r\n",
        "    text.translate(str.maketrans('', '', string.punctuation))\r\n",
        "\r\n",
        "    tokensText = nltk.word_tokenize(text) # tokenizacija dokumenata\r\n",
        "    stopwords=nltk.corpus.stopwords.words('english') # micanje stopwords (the, at ...)\r\n",
        "\r\n",
        "    tokensText=[w for w in tokensText if w.lower() not in stopwords]\r\n",
        "    tokensText=[w.lower() for w in tokensText] # lower case slova\r\n",
        "\r\n",
        "    tokensText=[w for w in tokensText if len(w)>2] # samo tokeni duljine >2\r\n",
        "    p= PorterStemmer() # uzimanje korijena rijeci algoritmom Porter Stemmer\r\n",
        "    tokensText = [p.stem(w) for w in tokensText]   \r\n",
        "\r\n",
        "    tokenIndices = []\r\n",
        "    counter = len(vocabulary) - 1\r\n",
        "\r\n",
        "    for token in tokensText:\r\n",
        "      if token not in vocabulary:\r\n",
        "        counter = counter + 1\r\n",
        "\r\n",
        "        vocabulary[token] = counter\r\n",
        "        tokenIndices.append(counter)\r\n",
        "      else:\r\n",
        "        tokenIndices.append(vocabulary[token])\r\n",
        "      \r\n",
        "    finalDocumentIndices.append(tokenIndices)\r\n",
        "   \r\n",
        "  return vocabulary, finalDocumentIndices \r\n",
        "\r\n",
        "def tokenizationWithVocabulary(data, vocabulary):\r\n",
        "  finalDocumentIndices = []\r\n",
        "\r\n",
        "  for i in data.keys():\r\n",
        "    text = data[i]\r\n",
        "    text.translate(str.maketrans('', '', string.punctuation))\r\n",
        "\r\n",
        "    tokensText = nltk.word_tokenize(text) \r\n",
        "    stopwords=nltk.corpus.stopwords.words('english') \r\n",
        "\r\n",
        "    tokensText=[w for w in tokensText if w.lower() not in stopwords]\r\n",
        "    tokensText=[w.lower() for w in tokensText] \r\n",
        "\r\n",
        "    tokensText=[w for w in tokensText if len(w)>2] \r\n",
        "    p= PorterStemmer() \r\n",
        "    tokensText = [p.stem(w) for w in tokensText]   \r\n",
        "\r\n",
        "    tokenIndices = []\r\n",
        "\r\n",
        "    for token in tokensText:\r\n",
        "      if token in vocabulary:\r\n",
        "        tokenIndices.append(vocabulary[token])\r\n",
        "    finalDocumentIndices.append(tokenIndices)\r\n",
        "  return finalDocumentIndices"
      ],
      "execution_count": 476,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpphwMyBpP_R"
      },
      "source": [
        "# **Feature extraction and TFIDF matrix construction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT9VWYlepQGR"
      },
      "source": [
        "def features(finalDocumentIndices, frequencyTreshold, dataframeClass):\r\n",
        "  \r\n",
        "  wordFrequency = {}\r\n",
        "  hamCounter = 0\r\n",
        "  spamCounter = 0\r\n",
        "\r\n",
        "  for k,document in enumerate(finalDocumentIndices):\r\n",
        "    k = k + dataframeClass.keys().start\r\n",
        "    if dataframeClass[k] == \"ham\":\r\n",
        "      hamCounter = hamCounter + 1 \r\n",
        "\r\n",
        "    if dataframeClass[k] == \"ham\":\r\n",
        "      continue\r\n",
        "\r\n",
        "    for index in document:\r\n",
        "      if index in wordFrequency:\r\n",
        "        wordFrequency[index] = wordFrequency[index] + 1\r\n",
        "      else:\r\n",
        "        wordFrequency[index] = 1\r\n",
        "\r\n",
        "      mostFrequentWords = []\r\n",
        "\r\n",
        "      for tokenIndex in wordFrequency.keys():\r\n",
        "        if wordFrequency[tokenIndex] > frequencyTreshold:\r\n",
        "            mostFrequentWords.append(tokenIndex)\r\n",
        "\r\n",
        "  mostFrequentWordsDict = {}\r\n",
        "  i = 0\r\n",
        "\r\n",
        "  for tokenIndex in mostFrequentWords:\r\n",
        "    mostFrequentWordsDict[i] = tokenIndex\r\n",
        "    i = i + 1\r\n",
        "\r\n",
        "  return mostFrequentWordsDict\r\n",
        "  \r\n",
        "\r\n",
        "def createTfidfMatrix(mostFrequentWordsDict, finalDocumentIndices):\r\n",
        "  featureIndices = mostFrequentWordsDict.values()\r\n",
        "\r\n",
        "  rows = []\r\n",
        "  rows = list(featureIndices)\r\n",
        "  columns = []\r\n",
        "\r\n",
        "  for listIndices in finalDocumentIndices:\r\n",
        "    featureVector = [0] * (len(featureIndices))\r\n",
        "\r\n",
        "    for i in listIndices:\r\n",
        "      if i in rows:\r\n",
        "        featureVector[rows.index(i)] = listIndices.count(i)\r\n",
        "\r\n",
        "    columns.append(featureVector)\r\n",
        "\r\n",
        "  tfidf = TfidfTransformer(norm=False,use_idf=True,sublinear_tf=True, smooth_idf=True)\r\n",
        "  tfidf.fit(columns)\r\n",
        "  tfidfMatrix = tfidf.transform(columns)\r\n",
        "\r\n",
        "  return tfidfMatrix"
      ],
      "execution_count": 477,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilVLBpShFoTa"
      },
      "source": [
        "# **Reading data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kwny_4JFobT"
      },
      "source": [
        "dataframe = pd.read_csv('spam.csv', encoding= \"ISO-8859-1\")\r\n",
        "\r\n",
        "dataframe = dataframe.iloc[:,:2]\r\n",
        "\r\n",
        "dataframe.columns = [\"Class\",\"Content\"]\r\n",
        "dataframe[\"Class_number\"] = pd.factorize(dataframe[\"Class\"])[0]\r\n",
        "\r\n",
        "#char -> int\r\n",
        "def contentToIntegers(dataframeRow):\r\n",
        "  return [ord(character) for character in dataframeRow[\"Content\"]]\r\n",
        "\r\n",
        "dataframe[\"Content_integers\"] = dataframe.apply(contentToIntegers, axis=1)\r\n",
        "\r\n",
        "#naci maxInteger iz Content_integers za skaliranje na [0,1]\r\n",
        "maxInteger = -math.inf\r\n",
        "for i in range(len(dataframe[\"Content_integers\"])):\r\n",
        "  for j in range(len(dataframe[\"Content_integers\"][i])):\r\n",
        "    if dataframe[\"Content_integers\"][i][j] > maxInteger:\r\n",
        "      maxInteger = dataframe[\"Content_integers\"][i][j]\r\n",
        "\r\n",
        "X = dataframe[\"Content_integers\"].values\r\n",
        "\r\n",
        "for i in range(len(dataframe)):\r\n",
        "  X[i] = np.asarray(X[i])\r\n",
        "\r\n",
        "y = dataframe[\"Class_number\"].values\r\n",
        "y = y.reshape(len(y), 1)\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\r\n",
        "\r\n",
        "maxLength = 100\r\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxLength) #ujednacavanje duljine poruka\r\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxLength)\r\n",
        "\r\n",
        "X_train = X_train / maxInteger #skaliranje na [0,1]\r\n",
        "X_test = X_test / maxInteger\r\n",
        "\r\n",
        "# reshape\r\n",
        "X_test = X_test.T\r\n",
        "X_train = X_train.T\r\n",
        "y_test = y_test.T\r\n",
        "y_train = y_train.T"
      ],
      "execution_count": 478,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfMJZ_DMFvdR"
      },
      "source": [
        "# **K-means**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCfd04HkFvlR"
      },
      "source": [
        "def kMeans(X_train,y_train, X_test, y_test):\r\n",
        "  print('Kmeans algoritam:')\r\n",
        "  X_train = X_train.T\r\n",
        "  X_test = X_test.T\r\n",
        "  kmeans = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 1000, tol = 1e-5, copy_x = False).fit(X_train)\r\n",
        "  trainLabels = kmeans.labels_ #(procitaj oznake)\r\n",
        "  \r\n",
        "  testPredict = kmeans.predict(X_test)\r\n",
        "\r\n",
        "  numberOfZeros = 0\r\n",
        "  numberOfOnes = 0\r\n",
        "  for i in range(X_test.shape[0]):\r\n",
        "    if testPredict[i] == 0:\r\n",
        "      if y_test[0][i] == 0:\r\n",
        "        numberOfZeros = numberOfZeros + 1\r\n",
        "      else:\r\n",
        "        numberOfOnes = numberOfOnes + 1\r\n",
        "\r\n",
        "  print(\"Prva klasa:\")\r\n",
        "  print('Broj \"ham\" SMS poruka:', numberOfZeros)\r\n",
        "  print('Broj \"spam\" SMS poruka:', numberOfOnes)\r\n",
        "\r\n",
        "  numberOfZeros = 0\r\n",
        "  numberOfOnes = 0\r\n",
        "\r\n",
        "  for i in range(X_test.shape[0]):\r\n",
        "    if testPredict[i] == 1:\r\n",
        "      if y_test[0][i] == 0:\r\n",
        "        numberOfZeros = numberOfZeros + 1\r\n",
        "      else:\r\n",
        "        numberOfOnes = numberOfOnes + 1\r\n",
        "  print(\"Druga klasa:\")\r\n",
        "  print('Broj \"ham\" SMS poruka:', numberOfZeros)\r\n",
        "  print('Broj \"spam\" SMS poruka:', numberOfOnes)\r\n",
        "\r\n"
      ],
      "execution_count": 479,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yq_xy03D3ae"
      },
      "source": [
        "# **K-nearest neighbors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6u5inaCD3lW"
      },
      "source": [
        "def kNearestNeighbors(X_train,y_train, X_test, y_test):\r\n",
        "  print('KNN algoritam:')\r\n",
        "  X_train = X_train.T\r\n",
        "  y_train = np.squeeze(y_train)\r\n",
        "  X_test = X_test.T\r\n",
        "  y_test = np.squeeze(y_test)\r\n",
        "  knn = KNeighborsClassifier(n_neighbors = 2).fit(X_train, y_train)\r\n",
        "\r\n",
        "  trainLabels = []\r\n",
        "  testPredict = knn.predict(X_test)\r\n",
        "\r\n",
        "  numberOfZeros = 0\r\n",
        "  numberOfOnes = 0\r\n",
        "  for i in range(X_test.shape[0]):\r\n",
        "    if testPredict[i] == 0:\r\n",
        "      if y_test[i] == 0:\r\n",
        "        numberOfZeros = numberOfZeros + 1\r\n",
        "      else:\r\n",
        "        numberOfOnes = numberOfOnes + 1\r\n",
        "  print(\"Prva klasa:\")\r\n",
        "  print('Broj \"ham\" SMS poruka:', numberOfZeros)\r\n",
        "  print('Broj \"spam\" SMS poruka:', numberOfOnes)\r\n",
        "\r\n",
        "  numberOfZeros = 0\r\n",
        "  numberOfOnes = 0\r\n",
        "\r\n",
        "  for i in range(X_test.shape[0]):\r\n",
        "    if testPredict[i] == 1:\r\n",
        "      if y_test[i] == 0:\r\n",
        "        numberOfZeros = numberOfZeros + 1\r\n",
        "      else:\r\n",
        "        numberOfOnes = numberOfOnes + 1\r\n",
        "  print(\"Druga klasa:\")\r\n",
        "  print('Broj \"ham\" SMS poruka:', numberOfZeros)\r\n",
        "  print('Broj \"spam\" SMS poruka:', numberOfOnes)\r\n"
      ],
      "execution_count": 480,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP4HndJXMcqr"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lETsA0WyMc00",
        "outputId": "3fd0da70-21ed-433a-8aa6-c9421518ea3a"
      },
      "source": [
        "# Randomiziranje uzorka\r\n",
        "\r\n",
        "numberOfMessages = dataframe['Class'].shape[0]\r\n",
        "def randomChoices(number, trainPercent):\r\n",
        "\r\n",
        "  percentage = int(trainPercent * number)\r\n",
        "\r\n",
        "  randomTrain = np.random.choice(number, percentage, replace = False)\r\n",
        "\r\n",
        "  randomTest = []\r\n",
        "  for i in range(number):\r\n",
        "    if i not in randomTrain:\r\n",
        "      randomTest.append(i)\r\n",
        "  return randomTrain, randomTest\r\n",
        "\r\n",
        "randomTrain, randomTest = randomChoices(numberOfMessages, 0.7)\r\n",
        "randomizedOrder = np.concatenate((randomTrain,randomTest))\r\n",
        "\r\n",
        "tempDataframe = dataframe.copy()\r\n",
        "tempy = y.copy()\r\n",
        "\r\n",
        "for i in range(numberOfMessages):\r\n",
        "  dataframe.iloc[i] = tempDataframe.iloc[randomizedOrder[i]]\r\n",
        "  y[i][0] = tempy[randomizedOrder[i]][0]\r\n",
        "\r\n",
        "vocabulary, finalDocumentIndices = tokenization(dataframe[\"Content\"][0:3899])\r\n",
        "\r\n",
        "mostFrequentWordsDict = features(finalDocumentIndices, 12, dataframe[\"Class\"][0:3899])\r\n",
        "\r\n",
        "matrix = createTfidfMatrix(mostFrequentWordsDict, finalDocumentIndices)\r\n",
        "\r\n",
        "\r\n",
        "finalDocumentIndicesTest = tokenizationWithVocabulary(dataframe[\"Content\"][3899:], vocabulary)\r\n",
        "\r\n",
        "matrixTest = createTfidfMatrix(mostFrequentWordsDict, finalDocumentIndicesTest)\r\n",
        "\r\n",
        "matrix = matrix.T\r\n",
        "y = y.T\r\n",
        "yTest = y[0][3899:].copy()\r\n",
        "yTest = yTest.T\r\n",
        "yTrain = y[0][:3899].copy()\r\n",
        "yTrain = yTrain.T\r\n",
        "matrixTest = matrixTest.T\r\n",
        "\r\n",
        "yTest = np.expand_dims(yTest, axis = 0)\r\n",
        "yTrain = np.expand_dims(yTrain, axis = 0)\r\n",
        "\r\n",
        "matrixOutput = matrix.toarray()\r\n",
        "matrixTestOutput = matrixTest.toarray()\r\n",
        "np.savetxt(\"X_trainTokenized.csv\", matrixOutput, delimiter=\",\")\r\n",
        "np.savetxt(\"X_testTokenized.csv\", matrixTestOutput, delimiter=\",\")\r\n",
        "np.savetxt(\"ShuffledLabels.csv\",y,delimiter=\",\")\r\n",
        "\r\n",
        "kMeans(X_train, y_train, X_test, y_test)\r\n",
        "print(\"\\n\")\r\n",
        "kMeans(matrix, yTrain, matrixTest, yTest)\r\n",
        "print(\"\\n\")\r\n",
        "kNearestNeighbors(X_train, y_train, X_test, y_test)\r\n",
        "print(\"\\n\")\r\n",
        "kNearestNeighbors(matrix, yTrain, matrixTest, yTest)"
      ],
      "execution_count": 481,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kmeans algoritam:\n",
            "Prva klasa:\n",
            "Broj \"ham\" SMS poruka: 882\n",
            "Broj \"spam\" SMS poruka: 12\n",
            "Druga klasa:\n",
            "Broj \"ham\" SMS poruka: 558\n",
            "Broj \"spam\" SMS poruka: 220\n",
            "\n",
            "\n",
            "Kmeans algoritam:\n",
            "Prva klasa:\n",
            "Broj \"ham\" SMS poruka: 13\n",
            "Broj \"spam\" SMS poruka: 148\n",
            "Druga klasa:\n",
            "Broj \"ham\" SMS poruka: 1426\n",
            "Broj \"spam\" SMS poruka: 86\n",
            "\n",
            "\n",
            "KNN algoritam:\n",
            "Prva klasa:\n",
            "Broj \"ham\" SMS poruka: 1427\n",
            "Broj \"spam\" SMS poruka: 146\n",
            "Druga klasa:\n",
            "Broj \"ham\" SMS poruka: 13\n",
            "Broj \"spam\" SMS poruka: 86\n",
            "\n",
            "\n",
            "KNN algoritam:\n",
            "Prva klasa:\n",
            "Broj \"ham\" SMS poruka: 1435\n",
            "Broj \"spam\" SMS poruka: 108\n",
            "Druga klasa:\n",
            "Broj \"ham\" SMS poruka: 4\n",
            "Broj \"spam\" SMS poruka: 126\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}